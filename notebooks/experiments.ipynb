{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Windows Lab: Experiment Analysis\n",
    "\n",
    "**Course:** LLMs and Multi-Agent Systems  \n",
    "**Team:** OmerAndYogever  \n",
    "**Assignment:** 5 - Context Windows in Practice\n",
    "\n",
    "This notebook demonstrates and analyzes four experiments exploring LLM context window behavior:\n",
    "\n",
    "1. **Needle in Haystack** - Lost in the Middle phenomenon\n",
    "2. **Context Size Impact** - Accuracy degradation with larger contexts\n",
    "3. **RAG Impact** - Comparing retrieval strategies\n",
    "4. **Context Engineering** - Managing context in multi-step agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install -e ..\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Project imports\n",
    "from context_windows_lab.config import Config\n",
    "from context_windows_lab.experiments import (\n",
    "    NeedleInHaystackExperiment,\n",
    "    ContextSizeExperiment,\n",
    "    RAGImpactExperiment,\n",
    "    ContextEngineeringExperiment,\n",
    ")\n",
    "from context_windows_lab.utils.document_generator import DocumentGenerator, FactPosition\n",
    "from context_windows_lab.utils.token_counter import TokenCounter\n",
    "from context_windows_lab.utils.statistics import StatisticalAnalyzer\n",
    "from context_windows_lab.utils.visualization import Visualizer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config.from_env()\n",
    "\n",
    "# Validate\n",
    "errors = config.validate()\n",
    "if errors:\n",
    "    print(\"⚠️ Configuration warnings:\")\n",
    "    for e in errors:\n",
    "        print(f\"  - {e}\")\n",
    "else:\n",
    "    print(f\"✓ Configuration valid\")\n",
    "    print(f\"  Model: {config.claude_model}\")\n",
    "    print(f\"  Trials per experiment: {config.num_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Needle in Haystack (Lost in the Middle)\n",
    "\n",
    "### Hypothesis\n",
    "LLMs have difficulty retrieving information placed in the middle of a long context, compared to information at the beginning or end.\n",
    "\n",
    "### Methodology\n",
    "- Generate synthetic documents with embedded facts\n",
    "- Place the critical fact at start, middle, or end positions\n",
    "- Measure retrieval accuracy for each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "needle_exp = NeedleInHaystackExperiment(\n",
    "    config=config,\n",
    "    verbose=True,\n",
    "    num_documents=5,\n",
    "    words_per_doc=200,\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {needle_exp.NAME}\")\n",
    "print(f\"Description: {needle_exp.DESCRIPTION}\")\n",
    "print(f\"Estimated duration: {needle_exp.ESTIMATED_DURATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "output_dir = Path('../outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "needle_result = needle_exp.run(num_trials=5, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1 RESULTS: Needle in Haystack\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis = needle_result.analysis\n",
    "print(f\"\\nMain Finding: {analysis['main_finding']}\")\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "for metric, value in analysis['key_metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nStatistical Significance: {analysis['statistical_significance']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if needle_result.visualizations:\n",
    "    display(Image(filename=needle_result.visualizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Context Window Size Impact\n",
    "\n",
    "### Hypothesis\n",
    "As context window size increases, LLM accuracy decreases and latency increases.\n",
    "\n",
    "### Methodology\n",
    "- Gradually increase the number of documents: 2, 5, 10, 20, 50\n",
    "- Measure accuracy, latency, and token usage at each level\n",
    "- Analyze correlation between size and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "size_exp = ContextSizeExperiment(\n",
    "    config=config,\n",
    "    verbose=True,\n",
    "    doc_counts=[2, 5, 10, 20, 50],\n",
    "    words_per_doc=200,\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {size_exp.NAME}\")\n",
    "print(f\"Description: {size_exp.DESCRIPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "size_result = size_exp.run(num_trials=5, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2 RESULTS: Context Size Impact\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis = size_result.analysis\n",
    "print(f\"\\nMain Finding: {analysis['main_finding']}\")\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "for metric, value in analysis['key_metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nAccuracy Correlation: r={analysis['accuracy_correlation']['r']:.3f}, p={analysis['accuracy_correlation']['p_value']:.4f}\")\n",
    "print(f\"Latency Correlation: r={analysis['latency_correlation']['r']:.3f}, p={analysis['latency_correlation']['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if size_result.visualizations:\n",
    "    display(Image(filename=size_result.visualizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: RAG Impact\n",
    "\n",
    "### Hypothesis\n",
    "RAG (Retrieval Augmented Generation) improves accuracy and reduces latency compared to full context.\n",
    "\n",
    "### Methodology\n",
    "- Create a corpus of 20 documents\n",
    "- Compare two approaches:\n",
    "  - **Full Context**: All documents in the context\n",
    "  - **RAG**: Only relevant documents retrieved via similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "rag_exp = RAGImpactExperiment(\n",
    "    config=config,\n",
    "    verbose=True,\n",
    "    total_documents=20,\n",
    "    relevant_documents=3,\n",
    "    words_per_doc=200,\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {rag_exp.NAME}\")\n",
    "print(f\"Description: {rag_exp.DESCRIPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "rag_result = rag_exp.run(num_trials=5, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3 RESULTS: RAG Impact\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis = rag_result.analysis\n",
    "print(f\"\\nMain Finding: {analysis['main_finding']}\")\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "for metric, value in analysis['key_metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nImprovements:\")\n",
    "print(f\"  Accuracy: {analysis['improvements']['accuracy']:.1%}\")\n",
    "print(f\"  Latency: {analysis['improvements']['latency']:.2f}s faster\")\n",
    "print(f\"  Tokens: {analysis['improvements']['tokens']:.0f} fewer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if rag_result.visualizations:\n",
    "    display(Image(filename=rag_result.visualizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Context Engineering Strategies\n",
    "\n",
    "### Hypothesis\n",
    "Different context management strategies (Select, Compress, Write) maintain performance better than naive accumulation.\n",
    "\n",
    "### Methodology\n",
    "- Simulate a multi-step agent executing 10 sequential actions\n",
    "- Compare four strategies:\n",
    "  - **Baseline**: Accumulate all history\n",
    "  - **Select**: Keep only recent items (RAG-like)\n",
    "  - **Compress**: Summarize when too long\n",
    "  - **Write**: Use external scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "eng_exp = ContextEngineeringExperiment(\n",
    "    config=config,\n",
    "    verbose=True,\n",
    "    num_actions=10,\n",
    "    action_output_words=100,\n",
    "    max_context_tokens=2000,\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {eng_exp.NAME}\")\n",
    "print(f\"Description: {eng_exp.DESCRIPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "eng_result = eng_exp.run(num_trials=5, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 4 RESULTS: Context Engineering Strategies\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis = eng_result.analysis\n",
    "print(f\"\\nMain Finding: {analysis['main_finding']}\")\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "for metric, value in analysis['key_metrics'].items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Strategy: {analysis['best_strategy'].upper()}\")\n",
    "print(f\"\\nANOVA Results:\")\n",
    "print(f\"  F-statistic: {analysis['anova']['statistic']:.2f}\")\n",
    "print(f\"  p-value: {analysis['anova']['p_value']:.4f}\")\n",
    "print(f\"  Effect size (η²): {analysis['anova']['effect_size']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if eng_result.visualizations:\n",
    "    display(Image(filename=eng_result.visualizations[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Conclusions\n",
    "\n",
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = [\n",
    "    {\n",
    "        \"Experiment\": \"1. Needle in Haystack\",\n",
    "        \"Key Finding\": needle_result.analysis.get('main_finding', 'N/A')[:100] + \"...\",\n",
    "        \"Significant\": \"Yes\" if any('significant' in str(v).lower() for v in needle_result.analysis.values()) else \"No\",\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"2. Context Size Impact\",\n",
    "        \"Key Finding\": size_result.analysis.get('main_finding', 'N/A')[:100] + \"...\",\n",
    "        \"Significant\": \"Yes\" if size_result.analysis.get('accuracy_correlation', {}).get('significant', False) else \"No\",\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"3. RAG Impact\",\n",
    "        \"Key Finding\": rag_result.analysis.get('main_finding', 'N/A')[:100] + \"...\",\n",
    "        \"Significant\": \"Yes\" if rag_result.analysis.get('comparisons', {}).get('accuracy', {}).get('significant', False) else \"No\",\n",
    "    },\n",
    "    {\n",
    "        \"Experiment\": \"4. Context Engineering\",\n",
    "        \"Key Finding\": eng_result.analysis.get('main_finding', 'N/A')[:100] + \"...\",\n",
    "        \"Significant\": \"Yes\" if eng_result.analysis.get('anova', {}).get('significant', False) else \"No\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Lost in the Middle**: Information placed in the middle of long contexts is harder to retrieve. This is a fundamental limitation of transformer attention mechanisms.\n",
    "\n",
    "2. **Context Size Trade-offs**: Larger contexts provide more information but at the cost of accuracy and latency. There's an optimal context size for each task.\n",
    "\n",
    "3. **RAG Benefits**: Retrieval-Augmented Generation significantly improves both accuracy and efficiency by focusing on relevant information.\n",
    "\n",
    "4. **Context Engineering**: Strategic management of context (Select, Compress, Write) helps maintain performance in multi-step agent workflows.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- Place critical information at the **beginning or end** of prompts\n",
    "- Use **RAG** for large document collections\n",
    "- Implement **context management strategies** for agents with long conversations\n",
    "- Monitor **token usage** to optimize cost and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAll results saved to: {output_dir.absolute()}\")\n",
    "print(f\"\\nFiles generated:\")\n",
    "for f in output_dir.glob(\"*.png\"):\n",
    "    print(f\"  - {f.name}\")\n",
    "for f in output_dir.glob(\"*.json\"):\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
